{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Fake Tweets (*R*)\n",
    "\n",
    "In this notebooks, we will use the \"fake_followers.csv\" data file, from the dataset provided by the Fake Project, as our source data file.\n",
    "\n",
    "## Load the data file\n",
    "\n",
    "> First, we store the file name (*and its location*) in the string '**fileName**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName0 = 'datasetsFULLcsv/fakeFollowersCSV/tweets.csv'\n",
    "\n",
    "fileNames = c('datasetsFULLcsv/socialSpambots1csv/tweets.csv', 'datasetsFULLcsv/socialSpambots2csv/tweets.csv', 'datasetsFULLcsv/socialSpambots3csv/tweets.csv', 'datasetsFULLcsv/traditionalSpambots1csv/tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using the CSV filename previously specified in '**fileName**', we can now load the file into the _data.frame_( ) named '**fakeCSV**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”"
     ]
    }
   ],
   "source": [
    "fakeCSV = read.csv(fileName0)\n",
    "fakeTweets <- data.frame(userID = fakeCSV$user_id, id = fakeCSV$id, text = fakeCSV$text)\n",
    "\n",
    "for (filename in fileNames) {\n",
    "    temp0 = read.csv(filename)\n",
    "    #fakeCSV <- rbind(fakeCSV, temp0)\n",
    "    temp <- data.frame(userID = temp0$user_id, id = temp0$id, text = temp0$text)\n",
    "    fakeTweets <- rbind(fakeTweets, temp)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the '**fakeCSV**' _data.frame_( ), we will create a smaller, simpler *data.frame*( ) named '**fakeTweets**'.  This reduction in size and complexity of '**fakeTweets**' is due to the fact that it only contains the ID number of the tweet in our database, the ID number of the user who generated the tweet, along with the text of the tweet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fakeTweets <- data.frame(userID = fakeCSV$user_id, id = fakeCSV$id, text = fakeCSV$text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we remove web URLS, twitter usernames, twitter hashtags, punctuation, and stand-alone numeric digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove web URLs\n",
    "fakeTweets <- data.frame(userID = fakeTweets$userID, id = fakeTweets$id, text = gsub(\"http[[:alnum:][:punct:]]*\", \"\", fakeTweets$text))\n",
    "\n",
    "# remove twitter handles (@<username>)\n",
    "fakeTweets <- data.frame(userID = fakeTweets$userID, id = fakeTweets$id, text = gsub(\"#[[:alnum:][:punct:]]*\", \"\", fakeTweets$text))\n",
    "\n",
    "# remove hashtags (#<hashtag name>)\n",
    "fakeTweets <- data.frame(userID = fakeTweets$userID, id = fakeTweets$id, text = gsub(\"@[[:alnum:][:punct:]]*\", \"\", fakeTweets$text))\n",
    "\n",
    "# remove punctuation\n",
    "fakeTweets <- data.frame(userID = fakeTweets$userID, id = fakeTweets$id, text = gsub('[[:punct:] ]+', ' ', fakeTweets$text))\n",
    "\n",
    "# remove numbers\n",
    "fakeTweets <- data.frame(userID = fakeTweets$userID, id = fakeTweets$id, text = gsub(\"[0-9]\", \"\", fakeTweets$text))\n",
    "\n",
    "# convert to lowercase\n",
    "fakeTweets <- data.frame(userID = fakeTweets$userID, id = fakeTweets$id, text = tolower(fakeTweets$text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TidyText the data file\n",
    "\n",
    "> Now we must tokenize the text of each tweet using the '*tidytext*' and '*dplyr*' libraries.  First, we must import the '*tidytext*' and '*dplyr*' libraries,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(dplyr)\n",
    "library(tidytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then we convert the data frame of '**fakeTweets**' to the type from the '*dplyr*' library,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fakeTweets <- data_frame(userID = fakeTweets$userID, id = fakeTweets$id, text = as.character(fakeTweets$text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> so that we can finally tokenize the text from each of the tweets,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeTweetsTOKENS <- fakeTweets %>%\n",
    "    unnest_tokens(word, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove '*Stop Words*'\n",
    "\n",
    "> Now, we will remove any stop words from the text of the tweets.  To do this, we first import the '*stop_words*' dataset from the '*tidytext*' library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, we use the '*anti_join*( )' function from the '*dplyr*' library to remove these stop wrods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"word\"\n"
     ]
    }
   ],
   "source": [
    "fakeTweetsTOKENS <- fakeTweetsTOKENS %>%\n",
    "    anti_join(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrcWORDS <- get_sentiments(\"nrc\")\n",
    "nrcEMOTIONS <- unique(nrcWORDS$sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"word\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n"
     ]
    }
   ],
   "source": [
    "fakeTweetsNRCsentiment <- data.frame(id = 0)\n",
    "for (emotion in nrcEMOTIONS){\n",
    "    fakeTweetsNRCsentiment0 <- inner_join(fakeTweetsTOKENS, filter(nrcWORDS, sentiment == emotion))\n",
    "    fakeTweetsNRCsentiment <- full_join(fakeTweetsNRCsentiment, fakeTweetsNRCsentiment0)\n",
    "    }\n",
    "fakeTweetsNRCsentiment <- data.frame(fakeTweetsNRCsentiment[-1,])\n",
    "#fakeTweetsNRCsentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attach(fakeTweetsNRCsentiment)\n",
    "fakeNRCscoredTweets <- data.frame(table(id, sentiment), realFAKEcat = \"fake\")\n",
    "detach(fakeTweetsNRCsentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'trust'</li>\n",
       "\t<li>'fear'</li>\n",
       "\t<li>'negative'</li>\n",
       "\t<li>'sadness'</li>\n",
       "\t<li>'anger'</li>\n",
       "\t<li>'surprise'</li>\n",
       "\t<li>'positive'</li>\n",
       "\t<li>'disgust'</li>\n",
       "\t<li>'joy'</li>\n",
       "\t<li>'anticipation'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'trust'\n",
       "\\item 'fear'\n",
       "\\item 'negative'\n",
       "\\item 'sadness'\n",
       "\\item 'anger'\n",
       "\\item 'surprise'\n",
       "\\item 'positive'\n",
       "\\item 'disgust'\n",
       "\\item 'joy'\n",
       "\\item 'anticipation'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'trust'\n",
       "2. 'fear'\n",
       "3. 'negative'\n",
       "4. 'sadness'\n",
       "5. 'anger'\n",
       "6. 'surprise'\n",
       "7. 'positive'\n",
       "8. 'disgust'\n",
       "9. 'joy'\n",
       "10. 'anticipation'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"trust\"        \"fear\"         \"negative\"     \"sadness\"      \"anger\"       \n",
       " [6] \"surprise\"     \"positive\"     \"disgust\"      \"joy\"          \"anticipation\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fakeNRCscoredTweets\n",
    "nrcEMOTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoredFakeTweets <- data.frame()\n",
    "#fakeNRCscoredTweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trustScores <- data.frame(id = filter(fakeNRCscoredTweets, sentiment == \"trust\")$id, trust = filter(fakeNRCscoredTweets, sentiment == \"trust\")$Freq)\n",
    "fearScores <- data.frame(id = filter(fakeNRCscoredTweets, sentiment == \"fear\")$id, fear = filter(fakeNRCscoredTweets, sentiment == \"fear\")$Freq)\n",
    "negScores <- data.frame(id = filter(fakeNRCscoredTweets, sentiment == \"negative\")$id, negative = filter(fakeNRCscoredTweets, sentiment == \"negative\")$Freq)\n",
    "sadnessScores <- data.frame(id = filter(fakeNRCscoredTweets, sentiment == \"sadness\")$id, sadness = filter(fakeNRCscoredTweets, sentiment == \"sadness\")$Freq)\n",
    "angerScores <- data.frame(id = filter(fakeNRCscoredTweets, sentiment == \"anger\")$id, anger = filter(fakeNRCscoredTweets, sentiment == \"anger\")$Freq)\n",
    "surpriseScores <- data.frame(id = filter(fakeNRCscoredTweets, sentiment == \"surprise\")$id, surprise = filter(fakeNRCscoredTweets, sentiment == \"surprise\")$Freq)\n",
    "posScores <- data.frame(id = filter(fakeNRCscoredTweets, sentiment == \"positive\")$id, positive = filter(fakeNRCscoredTweets, sentiment == \"positive\")$Freq)\n",
    "disgustScores <- data.frame(id = filter(fakeNRCscoredTweets, sentiment == \"disgust\")$id, disgust = filter(fakeNRCscoredTweets, sentiment == \"disgust\")$Freq)\n",
    "joyScores <- data.frame(id = filter(fakeNRCscoredTweets, sentiment == \"joy\")$id, joy = filter(fakeNRCscoredTweets, sentiment == \"joy\")$Freq)\n",
    "anticipationScores <- data.frame(id = filter(fakeNRCscoredTweets, sentiment == \"anticipation\")$id, anticipation = filter(fakeNRCscoredTweets, sentiment == \"anticipation\")$Freq, realFAKEcat = filter(fakeNRCscoredTweets, sentiment == \"anticipation\")$realFAKEcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n"
     ]
    }
   ],
   "source": [
    "nrcSCORES <- full_join(trustScores, full_join(fearScores, full_join(negScores, full_join(sadnessScores, full_join(angerScores, full_join(surpriseScores, full_join(posScores, full_join(disgustScores, full_join(joyScores, anticipationScores)))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nrcSCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
